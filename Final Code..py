# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AW8hOBtw6VOVuqv2lsU_L-MGgu3CUHhz
"""

# let's import some important libraries.
import os
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit, OptimizeWarning
from sklearn.metrics import r2_score
#Sliding median technique is inspired from [Cruz et al. 2022]
#Light curve generation technique is similar to [Cruz et al. 2022]

#Add suitable path as per you pc. please follow this type of pattern  r"C:\Mousam\Mondal\Desktop\Data_Folder"
file_location =

# The text file which contains the data does have column names but I faced dificulties to read them that is why i generated my own column names, and saved the data accordingly.
column = ['kbjd', 'observed_flux', 'observed_flux_error', 'julian_day_time', 'normalized_flux', 'normalized_flux_error']

#Sliding Median function is inspired from [Cruz et al. 2022] . Which we have used to normalized the flux and later on to generate chi_square values and plots.
def sliding_mdian(arr, size_window):
    half_window_size = size_window // 2
    medians = []
    for i in range(len(arr)):
        if i < half_window_size:
            medians.append(np.median(arr[:i + half_window_size + 1]))
        elif i >= len(arr) - half_window_size:
            medians.append(np.median(arr[i - half_window_size:]))
        else:
            medians.append(np.median(arr[i - half_window_size:i + half_window_size + 1]))
    return medians


# Loop over all the files to find our data file which ends with _allLCs.txt .
for root, dirs, files in os.walk(file_location):
    for file in files:
        if file.endswith("_allLCs.txt"):
            file_path_f = os.path.join(root, file)
            df_1 = pd.read_csv(file_path_f, sep="\s+", skiprows=10, names=column, engine='python')  # we ha to skip some rows cause it's in a text file and info data was present.

            # Here I have used some basic data cleaning techniques like numeric value converter, dropping the null values , also we have used a standard deviation limit = STDV/5
            df_1.dropna(inplace=True)
            df_1["Reduced_JD"] = pd.to_numeric(df_1["kbjd"], errors="coerce")
            df_1.dropna(subset=["Reduced_JD"], inplace=True)
            stdv = np.std(df_1["observed_flux"])
            stdv_limit = stdv / 5

            # Replacing fluxes below stdv/5 by the median of their surrounding points.
            for i in range(len(df_1)):
                if np.abs(df_1["observed_flux"].iloc[i] - df_1["observed_flux"].iloc[i]) > stdv_limit:
                    aux = np.arange(i-10, i+11)
                    aux = aux[(aux >= 0) & (aux < len(df_1))]
                    df_1.loc[i, "observed_flux"] = np.median(df_1["observed_flux"].iloc[aux])

            # I generated a window size which has 4 quarters , and normalized the flux using sliding median technique.
            size_window = len(df_1) // 4
            df_1["Normalized_FLUX"] = df_1["observed_flux"] / sliding_mdian(df_1["observed_flux"], size_window)

            # Here we are cleaning the normalized flux which is generated by the sliding median function
            df_1.dropna(subset=["Normalized_FLUX"], inplace=True)

            # we have defined a box/bin size and based on that we have structured the chi_square_values using numpy function zero_like.
            bin_sizes = np.arange(10, 101, 1)
            chi_square_values = np.zeros_like(bin_sizes, dtype=float)

            for i, bz in enumerate(bin_sizes):
                flux_aux = np.zeros_like(df_1["Normalized_FLUX"])
                for j, flux in enumerate(df_1["Normalized_FLUX"]):
                    if j < bz + 1:
                        flux_aux[j] = np.median(df_1["Normalized_FLUX"][:j + bz])
                    elif j > len(df_1["Normalized_FLUX"]) - 1 - (bz + 1):
                        flux_aux[j] = np.median(df_1["Normalized_FLUX"][j - bz:])
                    else:
                        flux_aux[j] = np.median(df_1["Normalized_FLUX"][j - bz:j + bz + 1])
                chi_square_values[i] = np.sum(((df_1["Normalized_FLUX"] - flux_aux) ** 2) / df_1["observed_flux_error"] ** 2) # Chi_square formula is typical matheatical chi_square formula.

            # Save chi-square values in a text file with target name . Just taking the target name from initial _allLCs.txt file. In this way it's easier.
            target = os.path.basename(file_path_f).replace("_allLCs.txt", "")
            result_text = os.path.join(file_location, target + "_chi_square_val.txt")
            np.savetxt(result_text, chi_square_values, fmt="%.4f")

            #Now we are done with the chi_square values and moving towards light_curves generation. Where the data is saved in a file which ends with '_res.txt'.
            # Technique is similar to [Cruz et al. 2022] . And I have followed their methods.
            Pe = 0  #If '_res.txt' file is not found this will be the default value.
            T0 = 0
            for res_file in files:
                # Check the file which has '_res.txt' in the end
                if res_file.endswith("_res.txt"):
                    file_path_f = os.path.join(root, res_file)

                    #Apparently , i have to agian skip some lines. Though i could have done this in a different way but code will be longer in that case.
                    read_data = pd.read_csv(file_path_f, skiprows=7, nrows=1, header=None, engine='python')

                    # Also don't forget to split your values.
                    parts = read_data.iloc[0, 0].split()
                    if len(parts) == 2:
                        # Extract pe and t0
                        Pe = float(parts[0])
                        T0 = float(parts[1])
                        break

            tvar = df_1["julian_day_time"] - T0 + Pe / 4.           #Inspired from [Cruz et al. 2022]
            phase = (tvar / Pe) - np.floor(tvar / Pe)
            phase_000 = (tvar / Pe) % 1.0
            phase_025 = (tvar / Pe - 0.25) % 1.

            #Enough with the data, where are the results. Here it is ::::::

            #Chi-Square plot generation.
            plt.style.use('default')
            plt.figure(figsize=(6, 4))
            plt.plot(bin_sizes, chi_square_values , 'o', markersize=2, color='black')
            plt.grid(True)
            plt.xlabel("Bin(or Box) Size")
            plt.ylabel("Chi-Square_values")
            plt.title("Chi-Square versus Box Size for {}".format(file))
            plt.show()

            # Light Curve generation
            plt.style.use('default')
            plt.figure(figsize=(6, 4))
            plt.plot(phase.values, df_1["normalized_flux"].values, 'o', markersize=1, color='blue')
            plt.grid(True)
            plt.xlabel("_phase_")
            plt.ylabel("Norm Flux")
            plt.title("Normalized Light Curve")
            plt.show()
            plt.close()

#Finished with our chi_square plots and light_curves now we need to address how we are supposed to create an automated classification scheme?

#That's where the second part of the code comes.

#We have used a damping function and polynomial function to classsify.

#Let's use a damping function. The damping function we have used here is the most common damping function. Source : Wekipidia or any other Acoustics Physics book.
#x = input values. A= Amplitude , decay = decay factor. w = frequency and phi= phase.
def damping(x, A, w, phi, decay):
    return A * np.exp(-decay*x) * np.sin(w * x + phi)
# choosing different x values for different bin sizes
x_values = [(10,51,1),(10,71,1),(10,91,1)]
result_list = []

# Loop over all the folders in folder path
for fold_1, fold_2, folders in os.walk(file_location):
    for folder in folders:
        if folder.endswith("_chi_square_val.txt"):
            file_location = os.path.join(fold_1, folder)
            datapoint_y = np.loadtxt(file_location)
            datapoint_yy = np.std(datapoint_y)
            data_max=np.std(datapoint_y)
            data_len=len(datapoint_y)
            # We will be substraacting the mean from the baseline to align everything around centre.
            mean_baseline= sum(datapoint_y)/data_len
            data_len=len(datapoint_y)
            datapoint_y -= mean_baseline
            n_max=np.max(data_len)
            #n_ar=np.arange(data_len)
            data_y_mean = datapoint_y+ mean_baseline
            n_ar=np.arange(data_len)
            ampli_tude=np.max(datapoint_y)
            try:
                PO_damping = [ampli_tude, 0.1, 0, 0.01]   #Initial parametric estimations. amplitude is accurate as I have used numpy max function. Rest are assumptions.
                PO_PT_damping, _ = curve_fit(damping, n_ar, datapoint_y, p0=PO_damping, maxfev=100000)

                # let's fit a 4 degree polynomial function
                degree_0f_polynomial = 4    #Better represent the data than three degree polynomials.
                P0_PT_p0lynomial = np.polyfit(n_ar, datapoint_y, degree_0f_polynomial)

                # Generate predictions using the optimized parameters
                prediction_damping = damping(n_ar, *PO_PT_damping) + mean_baseline
                pred_poly = np.polyval(P0_PT_p0lynomial, n_ar) + mean_baseline

                # now it's time to check the fitting goodness
                damping_r_2 = r2_score(data_y_mean , prediction_damping)
                poly_r_2 = r2_score(data_y_mean , pred_poly)

                # let's compare the values to generate Binary_Classification
                if damping_r_2 > 0.8 and poly_r_2 <= 0.2 and damping_r_2 > poly_r_2:
                    Binary_Classification = "CONTACT"
                elif poly_r_2 > 0.8 and damping_r_2 > 0.5 and poly_r_2 > damping_r_2:
                    Binary_Classification = "DETACHED"
                elif damping_r_2 < 0.7 and poly_r_2 < 0.5:
                    Binary_Classification = "SEMI-DETACHED "
                else:
                    Binary_Classification = " DETACHED "

                # let's append the Binary_Classification results
                result_list.append({"File Name": folder.replace("_chi_square_val.txt", ""), "Classification": Binary_Classification,
                                "R-squared (Damping Sinusoidal)": damping_r_2,
                                "R-squared (Polynomial)": poly_r_2})

                # let's plot with fitted parameters
                plt.figure(figsize=(6, 4))
                plt.plot(np.arange(data_len), data_y_mean , 'go-', label="Original", markersize=4)
                plt.plot(np.arange(data_len), prediction_damping, 'r--', label="Damping function Fit", linewidth=1.5)
                plt.plot(np.arange(data_len), pred_poly, 'b-.', label=f"Poly Fit", linewidth=1.5)
                plt.legend(fontsize=8)
                plt.xlabel("Bin Size", fontsize=8)
                plt.ylabel("Chi2_Values", fontsize=8)

                plt.grid(True)
                plt.title(f"Kepler ID: {folder.replace('_chi_square_val.txt', '')}, Classification: {Binary_Classification}")
                plt.show()
            except (RuntimeError, OptimizeWarning):
                Binary_Classification = "Not_Found"